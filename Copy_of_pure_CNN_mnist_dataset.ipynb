{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of  pure_CNN_mnist_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monika2910/Give-Life_-Predict-Blood-Donations/blob/main/Copy_of_pure_CNN_mnist_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqIi4FqrOCuR"
      },
      "source": [
        "ROOT='/MyDrive/Deep_learning/CNN_parctice'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oMeQCnxO_rs"
      },
      "source": [
        "import os\n",
        "os.chdir(ROOT)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB-bo976BhSi",
        "outputId": "e92c6eb0-e073-413b-eb7d-c84bea81713b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKmIGaWhDs84"
      },
      "source": [
        "The MNIST dataset is one of the most common datasets used for image classification and accessible from many different sources.  \n",
        "It is a dataset of 60,000 small square 28×28 pixel grayscale images of handwritten single digits between 0 and 9.\n",
        "It is a widely used and deeply understood dataset and, for the most part, is “solved.” Top-performing models are deep learning convolutional neural networks that achieve a classification accuracy of above 99%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDKLD3JTQFNH"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IOzy-hIg9M4"
      },
      "source": [
        "# **Using CNN for classification**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20LnSQEwhmuD"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Activation,BatchNormalization,Dropout,MaxPooling2D\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67PqGu9dV3mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716034da-40d4-4ddb-fe79-03f2a92f74e8"
      },
      "source": [
        "(trainX, trainY) , (testX, testY) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykil1o8OkW1s"
      },
      "source": [
        "# reshape dataset to have a single channel\n",
        "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "testX = testX.reshape((testX.shape[0], 28, 28, 1))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3fvphV8igBl"
      },
      "source": [
        "# one hot encode target values\n",
        "trainY = to_categorical(trainY)\n",
        "testY = to_categorical(testY)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDqTJ7VyhO7L"
      },
      "source": [
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N1KiWt2T5gJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f15caa-7107-437e-8293-71ca7c98d719"
      },
      "source": [
        "trainY.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU_CnIX_BE4j"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz-NYTWtBH9U"
      },
      "source": [
        "model2 = keras.Sequential([\n",
        "    \n",
        "    layers.Conv2D(28, (3,3),padding='same',activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.Conv2D(32, (3,3),padding='same',activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3),padding='same',activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(32, (3,3),activation='relu'),\n",
        " \n",
        "    layers.Flatten(),\n",
        "    layers.Dense(100, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWTR5UqOBV38",
        "outputId": "0d20654d-edf2-44cb-ea12-ef8b34501438"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 28)        280       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        8096      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 5, 5, 32)          18464     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               80100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 126,446\n",
            "Trainable params: 126,446\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZNaqGFSEiUs"
      },
      "source": [
        "## Train a Pure CNN (only convolution layers) with less than 10000 trainable parameters using the MNIST Dataset having minimum validation accuracy of 99.40%.\n",
        "\n",
        "**Purpose:** \n",
        "\n",
        "make a model which is less intense in term of computation requirements, this can be achieved by reducing trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX92fet5npEp"
      },
      "source": [
        "model =keras.Sequential()\n",
        "model.add(layers.Conv2D(10, (3, 3) ,activation='relu',  input_shape=(28, 28, 1)))  #channal dimention 26*26*10    receptive =3*3\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(layers.Conv2D(10,(3,3) ,activation='relu'))         #channal=24*24*10         receptive=5*5\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(layers.Conv2D(10, (1, 1) ,activation='relu'))      #channal=24*24*10      receptive=5*5\n",
        "model.add(layers.MaxPooling2D(pool_size=(2 ,2)))             #12*12                          10*10\n",
        "\n",
        "\n",
        "model.add(layers.Conv2D(16,(3,3) ,activation='relu'))         #channal=10         receptive=12\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "#model.add(layers.Conv2D(10,(1,1) ,activation='relu'))        #channal=10    receptive=12\n",
        "#model.add(layers.MaxPooling2D(pool_size=(2 ,2)))             #channal=5              24\n",
        "\n",
        "\n",
        "model.add(layers.Conv2D(16,(3,3) ,activation='relu'))          #channal=3     receptive=26\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.1))\n",
        "\n",
        "model.add(layers.Conv2D(16,(3,3) ,activation='relu'))        #6                 22\n",
        "model.add(layers.MaxPooling2D(pool_size=(2 ,2)))  \n",
        "\n",
        "#model.add(layers.Conv2D(16,(3,3) ,activation='relu')) \n",
        "                      \n",
        "model.add(layers.Conv2D(10,(3,3)))                          \n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEyRXxxpt8A4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22531bd-36dc-41f3-8566-7499cee95107"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_32 (Conv2D)           (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 26, 26, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 26, 26, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 10)        910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 24, 24, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 24, 24, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 24, 24, 10)        110       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 1, 1, 10)          1450      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "module_wrapper_3 (ModuleWrap (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 8,874\n",
            "Trainable params: 8,770\n",
            "Non-trainable params: 104\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0-WyUAyFcCY"
      },
      "source": [
        "We use convolutional layers as feature extractors while you use Dense layers for generating the classification. \n",
        "\n",
        "Replacing the Dense layers with Max Pooling based model does the trick for reducing trainable parameter.\n",
        "\n",
        "we can see there are huge difference in trainable parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbS_0puMGJfd"
      },
      "source": [
        "**Tune the Learning Rate. Explore how different learning rates impact the model performance as compared to the baseline model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUZTt15Ooaxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444f3ba4-2a52-42ff-b757-c78c46d52d2a"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "#creatingnthe \"scheduler\" function with two agruments i.e learningrate and epoch\n",
        "\n",
        "def scheduler(epoch,learning_rate):\n",
        "  return round(0.003*1/(1+0.319*epoch), 20)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.003),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#model.fit(trainX, trainY, batch_size=64, verbose=1, validation_data=(testX, testY),scheduler=scheduler(20,0.003))\n",
        "model.fit(trainX, trainY, batch_size=128, epochs=20, verbose=1, validation_data=(testX, testY), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.003.\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0298 - accuracy: 0.9908 - val_loss: 0.0309 - val_accuracy: 0.9904\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.002274450341167551.\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.0221 - accuracy: 0.9924 - val_loss: 0.0320 - val_accuracy: 0.9915\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0018315018315018317.\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.0188 - accuracy: 0.9937 - val_loss: 0.0244 - val_accuracy: 0.9924\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0015329586101175269.\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.0154 - accuracy: 0.9948 - val_loss: 0.0227 - val_accuracy: 0.9929\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001318101933216169.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.0230 - val_accuracy: 0.9933\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0011560693641618498.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0127 - accuracy: 0.9959 - val_loss: 0.0246 - val_accuracy: 0.9931\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001029512697323267.\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.0138 - accuracy: 0.9953 - val_loss: 0.0223 - val_accuracy: 0.9935\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009279307145066501.\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.0218 - val_accuracy: 0.9938\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0008445945945945946.\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.0194 - val_accuracy: 0.9938\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0007749935417204857.\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0226 - val_accuracy: 0.9940\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0007159904534606206.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.0216 - val_accuracy: 0.9935\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0006653359946773121.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0105 - accuracy: 0.9963 - val_loss: 0.0227 - val_accuracy: 0.9933\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0006213753106876554.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 0.0201 - val_accuracy: 0.9940\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0005828638041577617.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.0217 - val_accuracy: 0.9937\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0005488474204171241.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.0229 - val_accuracy: 0.9936\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0005185825410544512.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0085 - accuracy: 0.9971 - val_loss: 0.0232 - val_accuracy: 0.9929\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000491480996068152.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0083 - accuracy: 0.9972 - val_loss: 0.0212 - val_accuracy: 0.9941\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0004670714619336759.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.0222 - val_accuracy: 0.9937\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0004449718184514981.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0219 - val_accuracy: 0.9943\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00042486899872539303.\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0073 - accuracy: 0.9973 - val_loss: 0.0218 - val_accuracy: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7238640a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vRLJ2F_G0-u"
      },
      "source": [
        "Achieved val accuracy=99.40%  with 8874 trainable parameter using learning rate=.003"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rf49SW9KVux"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}